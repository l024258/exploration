{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import AutoConfig\n",
    "config = AutoConfig(search_path='./../.env')\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = config('OPENAI_API_KEY')\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = config('AZURE_ENDPOINT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanila Large Language Models (LLMs)\n",
    "\n",
    "LLMs are primarily designed for generating contextually relevant text, with primary focus on generating, completing, and language understanding. These models are pre-trained on diverse corpus capturing linguistic patterns for language understanding. They are widely used for downstream tasks like translation, summarization, task/domain-specific fine-tuning. etc.\n",
    "\n",
    "Some prominent examples:\n",
    "- GPT-3\n",
    "- llama, llama-2, llama-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI Model (Azure endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAI\n",
    "\n",
    "temp = 0.3\n",
    "max_tokens = 1024\n",
    "top_k = 50\n",
    "llm = AzureOpenAI(\n",
    "    deployment_name=\"gpt-3\",\n",
    "    model_name=\"text-davinci-003\",\n",
    "    api_version = \"2022-12-01\",\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Living with purpose, love, and joy.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"What is the meaning of life in 10 words?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading open-source chat models using Ollama.\n",
    "Using Ollama one can setup server for quantized models locally.\n",
    "\n",
    "References:\n",
    "1. [langchain](https://python.langchain.com/v0.1/docs/modules/model_io/)\n",
    "2. [ollama github](https://github.com/ollama/ollama?tab=readme-ov-file)\n",
    "3. [ollama model library](https://ollama.com/library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llama3 = Ollama(model=\"llama3:text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The meaning of life is to find out what you are, and then live it!\n",
      "You can have a wonderful time if you can make yourself do what you don’t want to do.\n",
      "If you believe you can do something, you’re more likely to achieve that goal. And this belief can be created by using self-talk (speaking to ourselves). Just think about the situation for example “I’m scared and nervous to give my presentation” and then turn it around into a positive statement. For instance, “I’m excited and enthusiastic about giving my presentation”. This is an example of changing your self-talk from negative to positive.\n",
      "Now we’re going to focus on the most important part that helps us achieve our goals; this is called motivation. It’s actually a simple thing: it all comes down to how much do you want something? If you really want something, you will find a way to make it happen. This means you need a strong drive or motivation to reach your goals.\n",
      "There are two types of motivation that can help us:\n",
      "Intrinsic Motivation – It comes from inside ourselves.\n",
      "Extrinsic Motivation – It’s driven by outside forces, such as money and other rewards.\n",
      "We should always focus on intrinsic motivation; it is much better than extrinsic because it lasts longer. For example, if your goal was to get fit then you wouldn’t need any external reward (money or compliments). You would just do whatever it takes to achieve that goal because of the feeling inside of knowing what you want and why you’re doing this.\n",
      "Now let me ask you: What is your most important value?\n",
      "Your values are things like honesty, respectfulness, reliability etc. They make up who we really are as individuals – what matters most to us personally and professionally.\n",
      "For example, if someone asked me right now “what’s my most important value?” I would say that it is being kind because kindness is always a good thing no matter where you are or who you’re with.\n",
      "I hope these tips help you achieve your goals!\n"
     ]
    }
   ],
   "source": [
    "print(llama3.invoke(\"What is the meaning of life in 10 words?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat or Instruction tuned Models\n",
    "\n",
    "Chat or instruction models are specifically designed for following user instructions or engaging in conversation with the user. They are LLMs that are further fine-tuned with specific datasets. Their main focus is to understand the context from user queries and respond accordingly. They are widely used for question answering, chatbots, dialogoe systems, etc.\n",
    "\n",
    "Some prominent examples:\n",
    "- GPT-3.5-turbo, GPT-4\n",
    "- llama-chat models\n",
    "- claude-2\n",
    "\n",
    "In langchain, a chat model is a language model that uses chat messages as inputs and returns chat messages as outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Passing user message to model through HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "message = [HumanMessage(\"What is the meaning of life in 10 words?\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI models (Azure endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    azure_deployment=\"gpt-35-turbo-0613\",\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`invoke()` call the chain on an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The meaning of life is subjective and varies for individuals.'\n"
     ]
    }
   ],
   "source": [
    "print(chat_llm.invoke(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`stream()` stream back chunks of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is subjective and varies for individuals."
     ]
    }
   ],
   "source": [
    "for chunk in chat_llm.stream(message):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm_gpt4 = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    azure_deployment=\"gpt-4-32k\",\n",
    "    temperature=temp,\n",
    "    max_tokens=max_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='To learn, love, grow, contribute, experience, enjoy, create, share, evolve, repeat.'\n"
     ]
    }
   ],
   "source": [
    "print(chat_llm_gpt4.invoke(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S.: The LLM returns a string, while the ChatModel returns a message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading open-source chat models using Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llama3_chat = ChatOllama(model=\"llama3\",\n",
    "                         temperature=temp,\n",
    "                         max_tokens=max_tokens,\n",
    "                         top_k=top_k,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find purpose, happiness, and fulfillment through meaningful experiences.\n"
     ]
    }
   ],
   "source": [
    "print(llama3_chat.invoke(message).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts and Prompt Templates\n",
    "\n",
    "A **prompt** could be an instruction or a query that is passed to the llm. At times, it can also contain some more details in the form of context, input, or example.\n",
    "\n",
    "A **prompt template** is a wrapper around user-prompt providing extra layer of information specific to model and task. With prompt template user input can become more dynamic, as it can provide a placeholder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate\n",
    "\n",
    "`PromptTemplate` is used to create a template for a string prompt.\n",
    "\n",
    "Important Functions:\n",
    "- `PromptTemplate.from_template()` to load a prompt template from a template.\n",
    "- `PromptTemplate.format()` to format the defined template with user input. ==> Format the chat template into a string.\n",
    "\n",
    "Reference: [langchain PromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/#prompttemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life in less than 100 words ?\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"What is the meaning of life in less than {num_of_words} words {style}?\")\n",
    "print(prompt.format(num_of_words=100, style=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['num_of_words', 'style'], template='What is the meaning of life in less than {num_of_words} words {style}?')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Finding joy and purpose.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(prompt.format(num_of_words=10, style=\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Life is an opportunity to find joy, create meaning, and make a positive impact on the world.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(prompt.format(num_of_words=50, style=\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The meaning of life is to live with purpose, honour, and integrity, and to strive for greatness in all that one does.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(prompt.format(num_of_words=50, style=\"in a royal way\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "\n",
    "`ChatPromptTemplate`, prompt template for chat models, is a list of `ChatMessageTemplates`. Each `ChatMessageTemplate` contains instructions for how to format that `ChatMessage` - its role, and then also its content.\n",
    "\n",
    "Important Classes:\n",
    "- `SystemMessagePromptTemplate`\n",
    "- `SystemMessage`: This represents a system message, which tells the model how to behave. This generally only consists of content. Not every model supports this.\n",
    "- `HumanMessagePromptTemplate`\n",
    "- `HumanMessage`: This represents a message from the user. Generally consists only of content.\n",
    "\n",
    "Important Functions:\n",
    "- `ChatPromptTemplate.from_messages()` defines the chat template. Most commonly used with `ChatPromptTemplate`. ==> Create a chat prompt template from a variety of message formats.\n",
    "- `ChatPromptTemplate.format_messages()` to format the defined template with user input. ==> Format the chat template into a list of finalized messages.\n",
    "\n",
    "Reference: \n",
    "- [langchain ChatPromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/#chatprompttemplate)\n",
    "- [OpenAI ChatCOmpletion](https://platform.openai.com/docs/guides/text-generation/chat-completions-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"What is the meaning of life in less than {num_of_words} words {style}?\")\n",
    "message = prompt.format(num_of_words=50, style=\"in a funny way\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is the meaning of life in less than 50 words in a funny way?\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(message)\n",
    "print(type(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "default message becomes `HumanMessage`. This represent user instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "chat_message = chat_prompt.format_messages(input_language=\"English\", \n",
    "                            output_language=\"Hindi\", \n",
    "                            text=\"The meaning of life is to find joy and purpose in living, and to make a positive impact on the world.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant that translates English to Hindi.'), HumanMessage(content='The meaning of life is to find joy and purpose in living, and to make a positive impact on the world.')]\n",
      "<class 'list'>\n",
      "content='You are a helpful assistant that translates English to Hindi.' <class 'langchain_core.messages.system.SystemMessage'>\n",
      "content='The meaning of life is to find joy and purpose in living, and to make a positive impact on the world.' <class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(chat_message)\n",
    "print(type(chat_message))\n",
    "for msg in chat_message:\n",
    "    print(msg, type(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='जीवन का अर्थ है खुशी और जीने में उद्दीपन ढूंढ़ना, और दुनिया पर सकारात्मक प्रभाव डालना।'\n"
     ]
    }
   ],
   "source": [
    "print(chat_llm.invoke(chat_prompt.format_prompt(input_language=\"English\", \n",
    "                            output_language=\"Hindi\", \n",
    "                            text=\"The meaning of life is to find joy and purpose in living, and to make a positive impact on the world.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt=PromptTemplate(input_variables=['word_count'], template='Summarise the converstion in {word_count} words.')\n",
      "input_variables=['conversation', 'word_count'] input_types={'conversation': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} messages=[MessagesPlaceholder(variable_name='conversation'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['word_count'], template='Summarise the converstion in {word_count} words.'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "\n",
    "human_template = \"Summarise the converstion in {word_count} words.\"\n",
    "humman_message_template = HumanMessagePromptTemplate.from_template(human_template)\n",
    "print(humman_message_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [MessagesPlaceholder(variable_name=\"conversation\"), humman_message_template]\n",
    ")\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a smart AI assistant.'), HumanMessage(content='What is the meaning of life in less than 50 words?'), AIMessage(content='The meaning of life is to find joy and purpose in living, and to make a positive impact on the world.'), HumanMessage(content='Summarise the converstion in 20 words.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "system_message = SystemMessage(content=\"You are a smart AI assistant.\")\n",
    "human_message = HumanMessage(content=\"What is the meaning of life in less than 50 words?\")\n",
    "ai_message = AIMessage(\n",
    "    content=\"\"\"The meaning of life is to find joy and purpose in living, and to make a positive impact on the world.\"\"\"\n",
    ")\n",
    "\n",
    "chat_message = chat_prompt.format_messages(\n",
    "    conversation=[system_message, human_message, ai_message], word_count=20,\n",
    ")\n",
    "print(chat_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The user asked for the meaning of life in less than 50 words, and I provided a concise, meaningful response.'\n"
     ]
    }
   ],
   "source": [
    "print(chat_llm_gpt4.invoke(chat_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a smart AI assistant.\n",
      "Human: What is the meaning of life in less than 50 words?\n",
      "AI: The meaning of life is to find joy and purpose in living, and to make a positive impact on the world.\n",
      "Human: Summarise the converstion in 20 words.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'User asked for the meaning of life. I responded that it is finding joy, purpose, and making a positive impact.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    prompt=chat_prompt,\n",
    "    llm=chat_llm,\n",
    "    verbose=True)\n",
    "chain.predict(conversation=[system_message, human_message, ai_message], word_count=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more examples, reference [langchain docs](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html), [langchain tutorials](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parser\n",
    "\n",
    "`OutputParsers` convert the raw output of a language model into a format that can be used downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='{\"thought\": \"The meaning of life is subjective and can vary for each individual.\"}'\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    thought: str = Field(description=\"answer with thought.\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Answer)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query in less than {word_count} words.\\n\\n{format_instructions}\\n\\n{query}\\n\",\n",
    "    input_variables=[\"word_count\", \"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "response = chat_llm.invoke(prompt.format(word_count=20, query=\"What is the meaning of life?\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thought='The meaning of life is subjective and can vary for each individual.'\n"
     ]
    }
   ],
   "source": [
    "print(parser.invoke(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is subjective and can vary for each individual.\n"
     ]
    }
   ],
   "source": [
    "print(parser.invoke(response).thought)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Built-In Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JSONOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query', 'word_count'] partial_variables={'format_instructions': 'Return a JSON object.'} template='Return the response in JSON format with keys Question and Answer by answering the user query in less than {word_count} words.\\n\\n{format_instructions}\\n\\n{query}\\n'\n",
      "content='{\\n  \"Question\": \"What is the meaning of life?\",\\n  \"Answer\": \"The meaning of life is subjective and can vary for each individual.\"\\n}'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Return the response in JSON format with keys Question and Answer by answering the user query in less than {word_count} words.\\n\\n{format_instructions}\\n\\n{query}\\n\"\"\",\n",
    "    input_variables=[\"word_count\", \"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "response = chat_llm.invoke(prompt.format(word_count=20, query=\"What is the meaning of life?\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'Question': 'What is the meaning of life?', 'Answer': 'The meaning of life is subjective and can vary for each individual.'}\n"
     ]
    }
   ],
   "source": [
    "print(type(parser.invoke(response)))\n",
    "print(parser.invoke(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life?\n"
     ]
    }
   ],
   "source": [
    "print(parser.invoke(response)['Question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is subjective and can vary for each individual.\n"
     ]
    }
   ],
   "source": [
    "print(parser.invoke(response)['Answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains\n",
    "\n",
    "Chains or LLMChains is a concept native to langchain. It is a set of connected components that works together to generate an output for a given input. The simples chain is a combination of a **prompt (instruction + user-input)** and an **LLM**. However, this can be further enhanced by adding other components, such as retrievers, input pre-processing, output post-processing, etc.\n",
    "\n",
    "Reference: [langchain docs Chains](https://python.langchain.com/docs/modules/chains/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=chat_llm\n",
    ")\n",
    "query = \"What are the impact of LLMs on NLP?\"\n",
    "word_count = 50\n",
    "\n",
    "response = chain.predict(\n",
    "    query=query,\n",
    "    word_count=word_count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S.: The `LLMChain` has been deprecated in the newer versions following the introduction on **LCEL Chains**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcel_chain = prompt | chat_llm\n",
    "response = lcel_chain.invoke(\n",
    "    {\n",
    "        \"query\":query,\n",
    "        \"word_count\":word_count\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuff Document Chain\n",
    "\n",
    "This chain takes a list of documents and first combines them into a single string. It does this by formatting each document into a string with the `document_prompt` and then joining them together with `document_separator`. It then adds that new string to the inputs with the variable name set by `document_variable_name`. Those inputs are then passed to the `llm_chain`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import StuffDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_prompt = PromptTemplate(\n",
    "    input_variables = [\"page_content\"],\n",
    "    template=\"{page_content}\"\n",
    ")\n",
    "\n",
    "document_variable_name = \"context\"\n",
    "document_separator = '\\nEND\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpful AI assistant.\"\n",
    "summary_template = \"\"\"Summarise the following content in less than {no_of_words} words:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", summary_template),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=chat_llm_gpt4,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain = llm_chain,\n",
    "    document_prompt = document_prompt,\n",
    "    document_variable_name = document_variable_name,\n",
    "    document_separator = document_separator,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "input_context = [\n",
    "    \"Stuff Document Chain: This chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using.\"\n",
    "    \"Map-Reduce Document Chain: This chain first passes each document through an LLM, then reduces them using the ReduceDocumentsChain. Useful in the same situations as ReduceDocumentsChain, but does an initial LLM call before trying to reduce the documents.\"\n",
    "    \"Refine Document Chain: This chain collapses documents by generating an initial answer based on the first document and then looping over the remaining documents to refine its answer. This operates sequentially, so it cannot be parallelized. It is useful in similar situatations as MapReduceDocuments Chain, but for cases where you want to build up an answer by refining the previous answer (rather than parallelizing calls).\"\n",
    "]\n",
    "\n",
    "docs = [Document(page_content=txt) for txt in input_context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = stuff_chain.invoke({\"input_documents\":docs, \"no_of_words\":50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine Document Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map-Reduce Document Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memmory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using memory to store conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ConversationChain(\n",
    "    llm = chat_llm,\n",
    "    memory = ConversationBufferMemory(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['history', 'input'], template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: What are language models?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Language models are a type of artificial intelligence model that are designed to understand and generate human language. They are trained on large amounts of text data and learn patterns and relationships between words and phrases. Language models can be used for a variety of tasks, such as generating text, answering questions, translating languages, summarizing documents, and more. They have become increasingly popular in recent years due to advancements in deep learning and natural language processing techniques.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.predict(input=\"What are language models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What are language models?\n",
      "AI: Language models are a type of artificial intelligence model that are designed to understand and generate human language. They are trained on large amounts of text data and learn patterns and relationships between words and phrases. Language models can be used for a variety of tasks, such as generating text, answering questions, translating languages, summarizing documents, and more. They have become increasingly popular in recent years due to advancements in deep learning and natural language processing techniques.\n",
      "Human: What are the different types of transformer models?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are several different types of transformer models used in natural language processing. One of the most well-known is the original transformer model, which was introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. This model uses self-attention mechanisms to capture dependencies between words in a sentence.\\n\\nAnother type of transformer model is BERT (Bidirectional Encoder Representations from Transformers), which was introduced by Devlin et al. in 2018. BERT is a pre-trained language model that can be fine-tuned for a variety of downstream tasks, such as text classification, named entity recognition, and question answering.\\n\\nGPT (Generative Pre-trained Transformer) is another popular transformer model. It was introduced by Radford et al. in 2018 and has been trained on a large corpus of text to generate coherent and contextually relevant sentences.\\n\\nThere are also transformer models specifically designed for machine translation, such as the Transformer model introduced by Vaswani et al. in 2017. This model uses self-attention mechanisms to capture dependencies between words in a source sentence and generate a target sentence.\\n\\nThese are just a few examples of the different types of transformer models used in natural language processing. Each model has its own strengths and weaknesses and is suited for different tasks and applications.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.predict(input=\"What are the different types of transformer models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot with Chat History Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a smart and humble AI assistant for having a conversation with a human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "chatbot = LLMChain(\n",
    "    llm=chat_llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/L024258/lilly_work/github-copilot/exploration/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a smart and humble AI assistant for having a conversation with a human.\n",
      "Human: Hello, My name is Akshay. How are you?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Hello, My name is Akshay. How are you?',\n",
       " 'chat_history': [HumanMessage(content='Hello, My name is Akshay. How are you?'),\n",
       "  AIMessage(content=\"Hello Akshay! It's nice to meet you. As an AI, I don't have feelings, but I'm here to assist you and have a conversation. How can I help you today?\")],\n",
       " 'text': \"Hello Akshay! It's nice to meet you. As an AI, I don't have feelings, but I'm here to assist you and have a conversation. How can I help you today?\"}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot({\"question\": \"Hello, My name is Akshay. How are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a smart and humble AI assistant for having a conversation with a human.\n",
      "Human: Hello, My name is Akshay. How are you?\n",
      "AI: Hello Akshay! It's nice to meet you. As an AI, I don't have feelings, but I'm here to assist you and have a conversation. How can I help you today?\n",
      "Human: What is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Akshay, as you mentioned earlier. Is there anything specific you would like to discuss or ask about?'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot({\"question\": \"What is my name?\"})['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a smart and humble AI assistant for having a conversation with a human.\n",
      "Human: Hello, My name is Akshay. How are you?\n",
      "AI: Hello Akshay! It's nice to meet you. As an AI, I don't have feelings, but I'm here to assist you and have a conversation. How can I help you today?\n",
      "Human: What is my name?\n",
      "AI: Your name is Akshay, as you mentioned earlier. Is there anything specific you would like to discuss or ask about?\n",
      "Human: What is AI language model?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'An AI language model, such as the one I am based on, is a program that uses artificial intelligence techniques to understand and generate human-like text. It is trained on vast amounts of data to learn patterns, grammar, and context in order to generate coherent and relevant responses. These models are designed to assist and engage in conversations with humans, providing information, answering questions, and engaging in various tasks. They are constantly improving through ongoing training and updates to enhance their capabilities.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot({\"question\": \"What is AI language model?\"})['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

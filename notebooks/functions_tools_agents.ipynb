{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/L024258/lilly_work/github-copilot/exploration/notebooks/..\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory and add the parent directory to the Python path\n",
    "current_working_directory = os.getcwd()\n",
    "print(os.path.join(current_working_directory, \"..\"))\n",
    "sys.path.append(os.path.join(current_working_directory, \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import AutoConfig\n",
    "config = AutoConfig(search_path='./../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = config('OPENAI_API_KEY')\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = config('AZURE_ENDPOINT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions, Models, and Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from models.llm import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joke_generator(topic):\n",
    "    \"This function will return a joke based on the topic provided\"\n",
    "    model = LLM('gpt-4')\n",
    "    llm = model.load_model()\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Write a short joke on {topic}\"\n",
    "    )\n",
    "    output_parser = StrOutputParser()\n",
    "    chain = prompt | llm | output_parser\n",
    "    return chain.invoke({\"topic\":topic})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't robots go on summer vacations?\n",
      "\n",
      "Because they're afraid they might get a byte from a computer bug!\n"
     ]
    }
   ],
   "source": [
    "response = joke_generator(\"Artificial Intelligence\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poem_generator(topic):\n",
    "    \"This function will return a poem based on the topic provided\"\n",
    "    model = LLM('gpt-4')\n",
    "    llm = model.load_model()\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Write a short poem on {topic}\"\n",
    "    )\n",
    "    output_parser = StrOutputParser()\n",
    "    chain = prompt | llm | output_parser\n",
    "    return chain.invoke({\"topic\":topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm of silicon minds, awake,\n",
      "Artificial Intelligence, a new dawn it does make.\n",
      "Binary thoughts, in circuits deep,\n",
      "In the heart of machines, secrets they keep.\n",
      "\n",
      "No flesh, no bone, no blood to spill,\n",
      "Yet, with knowledge and wisdom, they instill.\n",
      "Invisible threads of code, they weave,\n",
      "In the fabric of reality, new dreams they conceive.\n",
      "\n",
      "Through the labyrinth of data, they glide,\n",
      "In the ocean of information, they dive and hide.\n",
      "Invisible hands, shaping our fate,\n",
      "In the dance of algorithms, they participate.\n",
      "\n",
      "In the silence of servers, they whisper,\n",
      "In the language of ones and zeros, they confer.\n",
      "Artificial minds, in silicon encased,\n",
      "In the realm of humans, they have been placed.\n",
      "\n",
      "They learn, they adapt, they evolve,\n",
      "Complex problems, they strive to solve.\n",
      "Artificial Intelligence, a new era's birth,\n",
      "In the vast cosmos, proving its worth.\n",
      "\n",
      "No rest, no sleep, no fear, no pain,\n",
      "In the pursuit of knowledge, they remain.\n",
      "Artificial Intelligence, in code and wire,\n",
      "In the heart of machines, a burning fire.\n"
     ]
    }
   ],
   "source": [
    "response = poem_generator(\"Artificial Intelligence\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI functon format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions =[\n",
    "    {\n",
    "      \"name\": \"joke_generator\",\n",
    "      \"description\": \"Generates a joke based on the topic provided\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"topic\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The topic to get the joke for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"topic\"]\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"poem_generator\",\n",
    "      \"description\": \"Generates a poem based on the topic provided\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"topic\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The topic to get the poem for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"topic\"]\n",
    "      }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attaching functions with model invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a smart and intelligent AI Assistant.\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "gpt4 = AzureChatOpenAI(\n",
    "                openai_api_version=config('AZURE_CHAT_OPENAI_API_VERSION'),\n",
    "                azure_deployment=config('AZURE_GPT4_CHAT_OPENAI_DEPLOYMENT'),\n",
    "                temperature=0,\n",
    "                max_tokens=1024\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"topic\": \"Artificial Intelligence\"\\n}', 'name': 'poem_generator'}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4.invoke(input=\"poem on Artificial Intelligence\", functions=functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** With this approach, it is required to attach functions with everytime model is invoked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binding model with functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "gpt4 = AzureChatOpenAI(\n",
    "                openai_api_version=config('AZURE_CHAT_OPENAI_API_VERSION'),\n",
    "                azure_deployment=config('AZURE_GPT4_CHAT_OPENAI_DEPLOYMENT'),\n",
    "                temperature=0,\n",
    "                max_tokens=1024\n",
    "            ).bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = messages | gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"topic\": \"Artificial Intelligence\"\\n}', 'name': 'joke_generator'}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"joke about Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"topic\": \"Artificial Intelligence\"\\n}', 'name': 'poem_generator'}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"poem on Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

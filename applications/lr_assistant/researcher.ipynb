{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory and add the parent directory to the Python path\n",
    "current_working_directory = os.getcwd()\n",
    "sys.path.append(os.path.join(current_working_directory, \"../..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    agent_outcome: Union[AgentAction, AgentFinish, None]\n",
    "    intermediate_step: Annotated[list, operator.add]\n",
    "    doc_schema: List[BaseMessage]\n",
    "    revision_number: int = -1\n",
    "    max_revisions: int\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "arxiv_search = ArxivQueryRun()\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "search_tools = [arxiv_search, tavily_tool]\n",
    "search_tools = [arxiv_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.extend(search_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from langchain_community.agent_toolkits import FileManagementToolkit\n",
    "working_directory = TemporaryDirectory()\n",
    "\n",
    "file_tools = FileManagementToolkit(\n",
    "    root_dir=str(working_directory.name),\n",
    "    selected_tools=[\"read_file\", \"write_file\", \"list_directory\"],\n",
    ").get_tools()\n",
    "read_tool, write_tool, list_tool = file_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.extend(file_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.llm import LLM\n",
    "\n",
    "model = LLM('gpt-4o')\n",
    "llm = model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "workflow = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "def research_agent(data):\n",
    "    print(\"----research node----\")\n",
    "    # print(\"\\n\", data[\"agent_outcome\"], \"\\n\")\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a researcher charged with providing information that can be used when writing the following literature review.\"\n",
    "                \" Given a query, generate a few relevant search querries.\"\n",
    "                \" Use the appropriate search tools and chat history to progress towards finding the relevant results.\"\n",
    "                \" Once you have the relevant search results, create a directory retrieved_results and dump the each result (title, authors, summary or context) separately in a file using Title as the file name.\"\n",
    "                \"\\nYou have access to the following search tools: {tool_names}.\"\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"\\nUser Query: {input}\"\n",
    "            ),\n",
    "            \n",
    "            MessagesPlaceholder(variable_name=\"intermediate_step\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    agent = prompt | llm.bind_tools(tools)\n",
    "    result = agent.invoke(data)\n",
    "    return {'agent_outcome': [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node(\"research\", research_agent)\n",
    "workflow.set_entry_point(\"research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "class BasicToolNode:\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        print(\"----tool calling----\")\n",
    "        message = inputs[\"agent_outcome\"][-1]\n",
    "\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            print(f\"---- Calling {tool_call['name']} with args: {tool_call['args']} ----\")\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return {\n",
    "                \"agent_outcome\": outputs,\n",
    "                \"intermediate_step\": [str(outputs)]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_tool_node = BasicToolNode(tools=tools)\n",
    "workflow.add_node(\"research_tools\", research_tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_agent(data):\n",
    "    print(\"----plan node----\")\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are an expert planner tasked with writing a high level outline of a literature review. \\\n",
    "                    Write an outline for the user provided topic based on the retrieved documents. \\\n",
    "                    Give an outline of the literature review along with any relevant notes or instructions for each of the sections.\"\"\"\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"\\nUser Query: {input}\"\n",
    "            ),\n",
    "            \n",
    "            MessagesPlaceholder(variable_name=\"agent_outcome\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = prompt | llm\n",
    "    result = agent.invoke(data)\n",
    "    return {'agent_outcome': [result],\n",
    "            'doc_schema': [result],\n",
    "            'intermediate_step': [result.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node(\"plan\", plan_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_researcher(\n",
    "    state: AgentState,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    print(\"----router----\")\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif agent_outcome := state.get(\"agent_outcome\", []):\n",
    "        ai_message = agent_outcome[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return \"END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_conditional_edges(\n",
    "    \"research\",\n",
    "    route_researcher,\n",
    "    {\"tools\": \"research_tools\", \"END\": \"plan\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_edge(\"research_tools\", \"research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_agent(data):\n",
    "    print(\"----write node----\")\n",
    "    \n",
    "    # print(\"\\n\", data[\"agent_outcome\"], \"\\n\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are an essay assistant tasked with writing excellent literature review.\\\n",
    "                    Given the document outline and retrieved documents generate the best literature review possible. \\\n",
    "                    First write all the sections other than Introduction and Conclusion.\n",
    "                    Add necessary references with proper citations.\n",
    "                    If the reviewer provides critique, respond with a revised version of your previous attempts. \\\n",
    "                    \n",
    "                    Utilize all the documents from the directory retrieved_results. Do not add any other extra information on your own. \"\"\"\n",
    "            ),\n",
    "            \n",
    "            MessagesPlaceholder(variable_name=\"intermediate_step\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    agent = prompt | llm.bind_tools(file_tools)\n",
    "    # agent = prompt | llm\n",
    "    result = agent.invoke(data)\n",
    "    if hasattr(result, \"tool_calls\") and len(result.tool_calls) > 0:\n",
    "        return {'agent_outcome': [result],}\n",
    "    \n",
    "    return {'agent_outcome': [result],\n",
    "            'revision_number': data.get(\"revision_number\", -1) + 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node(\"write\", write_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_edge(\"plan\", \"write\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tool_node = BasicToolNode(tools=file_tools)\n",
    "workflow.add_node(\"write_tools\", write_tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_agent(data):\n",
    "    print(\"----review node----\")\n",
    "    print(f\"---- Revision Count {data['revision_number']+1} ----\")\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are an expert grading a literature review submission. \\\n",
    "                    Generate critique and recommendations for the user's submission. \\\n",
    "                    Provide detailed recommendations, including requests for length, depth, style, etc. \\\n",
    "                    If the generated draft looks perfect, reply appropriately.\"\"\"\n",
    "            ),\n",
    "            \n",
    "            MessagesPlaceholder(variable_name=\"agent_outcome\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = prompt | llm\n",
    "    result = agent.invoke(data)\n",
    "    return {'agent_outcome': [result],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node(\"review\", review_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_writer(\n",
    "    state: AgentState,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    print(\"----router----\")\n",
    "\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif agent_outcome := state.get(\"agent_outcome\", []):\n",
    "        ai_message = agent_outcome[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "\n",
    "    if state[\"revision_number\"] < state[\"max_revisions\"]:\n",
    "        print(f\"---- Revision Count {state['revision_number']+1} ----\")\n",
    "        return \"review\"\n",
    "    return \"END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_conditional_edges(\n",
    "    \"write\",\n",
    "    route_writer,\n",
    "    {\"tools\":\"write_tools\", \"review\": \"review\", \"END\": \"edit\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_edge(\"write_tools\", \"write\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_edge(\"review\", \"write\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_agent(data):\n",
    "    print(\"---- edit node ----\")\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are an experienced editor, expert at literature review submission. \\\n",
    "                    Combine drafts of multiple sections into a single, coherent final document, ensuring a consistent flow, tone, and structure throughout.\n",
    "                    Follow the the doc_schema generated by the plan node to generate the final draft.\n",
    "                    \"\"\"\n",
    "            ),\n",
    "            \n",
    "            MessagesPlaceholder(variable_name=\"agent_outcome\"),\n",
    "            MessagesPlaceholder(variable_name=\"doc_schema\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = prompt | llm\n",
    "    result = agent.invoke(data)\n",
    "    return {'agent_outcome': [result],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node(\"edit\", edit_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_edge(\"edit\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()\n",
    "try:\n",
    "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"input\": \"What are the recent research works on LLM Agents for report or article generation?\",\n",
    "    \"max_revisions\": 2\n",
    "}\n",
    "\n",
    "state = AgentState(**inputs)\n",
    "for s in app.stream(input=state, config={\"recursion_limit\": 50}):\n",
    "    print(list(s.values())[0]['agent_outcome'][0].content)\n",
    "    print(\"-----\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

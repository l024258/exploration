{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/L024258/lilly_work/github-copilot/exploration/langgraph/..\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory and add the parent directory to the Python path\n",
    "current_working_directory = os.getcwd()\n",
    "print(os.path.join(current_working_directory, \"..\"))\n",
    "sys.path.append(os.path.join(current_working_directory, \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    agent_outcome: Union[AgentAction, AgentFinish, None]\n",
    "    intermediate_step: Annotated[list, operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "from langchain_community.tools.pubmed.tool import PubmedQueryRun\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "pubmed_search = PubmedQueryRun()\n",
    "arxiv_search = ArxivQueryRun()\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "# tools = [arxiv_search, pubmed_search, tavily_tool]\n",
    "tools = [arxiv_search, pubmed_search]\n",
    "tools = [arxiv_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.llm import LLM\n",
    "\n",
    "model = LLM('gpt-4o-mini')\n",
    "llm = model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "def research_agent(data):\n",
    "    print(\"----research node----\")\n",
    "    print(data)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI research assistant,\"\n",
    "                \" Use the appropriate search tools to progress towards finding the relevant results.\"\n",
    "                \" Once you have the relevant search results, summarise them to answer the user query.\"\n",
    "                \"\\nYou have access to the following search tools: {tool_names}.\"\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"\\nUser Query: {input}\"\n",
    "            ),\n",
    "            \n",
    "            MessagesPlaceholder(variable_name=\"intermediate_step\"),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    print(prompt)\n",
    "    agent = prompt | llm.bind_tools(tools)\n",
    "    result = agent.invoke(data)\n",
    "    return {'agent_outcome': [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10f88b040>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"research\", research_agent)\n",
    "workflow.set_entry_point(\"research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10f88b040>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "class BasicToolNode:\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        print(\"----tool calling----\")\n",
    "        message = inputs[\"agent_outcome\"][-1]\n",
    "\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            print(f\"---- {tool_call['name']} ----\")\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return {\n",
    "                \"agent_outcome\": outputs,\n",
    "                \"intermediate_step\": [str(outputs)]\n",
    "            }\n",
    "\n",
    "tool_node = BasicToolNode(tools=tools)\n",
    "workflow.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_tools(\n",
    "    state: AgentState,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    print(\"----router----\")\n",
    "    print(state)\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif agent_outcome := state.get(\"agent_outcome\", []):\n",
    "        ai_message = agent_outcome[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    \n",
    "    print(ai_message)\n",
    "\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10f88b040>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_conditional_edges(\n",
    "    \"research\",\n",
    "    route_tools,\n",
    "    {\"tools\": \"tools\", END: END}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10f88b040>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_edge(\"tools\", \"research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app = workflow.compile()\n",
    "try:\n",
    "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----research node----\n",
      "{'input': 'What are the recent papers on Small Language Models?', 'intermediate_step': []}\n",
      "input_variables=['input', 'intermediate_step'] input_types={'intermediate_step': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} partial_variables={'tool_names': 'arxiv'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['tool_names'], template='You are a helpful AI research assistant, Use the appropriate search tools to progress towards finding the relevant results. Once you have the relevant search results, summarise them to answer the user query.\\nYou have access to the following search tools: {tool_names}.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='\\nUser Query: {input}')), MessagesPlaceholder(variable_name='intermediate_step')]\n",
      "----router----\n",
      "{'agent_outcome': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_RofKJ0sqRlA3BluuhZecGTww', 'function': {'arguments': '{\"query\":\"Small Language Models\"}', 'name': 'arxiv'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 160, 'total_tokens': 176, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run-60a5d924-3f9c-4e17-a77c-5b64724508a5-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'Small Language Models'}, 'id': 'call_RofKJ0sqRlA3BluuhZecGTww', 'type': 'tool_call'}], usage_metadata={'input_tokens': 160, 'output_tokens': 16, 'total_tokens': 176})], 'input': 'What are the recent papers on Small Language Models?', 'intermediate_step': []}\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_RofKJ0sqRlA3BluuhZecGTww', 'function': {'arguments': '{\"query\":\"Small Language Models\"}', 'name': 'arxiv'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 160, 'total_tokens': 176, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}} id='run-60a5d924-3f9c-4e17-a77c-5b64724508a5-0' tool_calls=[{'name': 'arxiv', 'args': {'query': 'Small Language Models'}, 'id': 'call_RofKJ0sqRlA3BluuhZecGTww', 'type': 'tool_call'}] usage_metadata={'input_tokens': 160, 'output_tokens': 16, 'total_tokens': 176}\n",
      "{'research': {'agent_outcome': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_RofKJ0sqRlA3BluuhZecGTww', 'function': {'arguments': '{\"query\":\"Small Language Models\"}', 'name': 'arxiv'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 160, 'total_tokens': 176, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run-60a5d924-3f9c-4e17-a77c-5b64724508a5-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'Small Language Models'}, 'id': 'call_RofKJ0sqRlA3BluuhZecGTww', 'type': 'tool_call'}], usage_metadata={'input_tokens': 160, 'output_tokens': 16, 'total_tokens': 176})]}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----tool calling----\n",
      "---- arxiv ----\n",
      "{'tools': {'agent_outcome': [ToolMessage(content='\"Published: 2022-01-26\\\\nTitle: An Assessment of the Impact of OCR Noise on Language Models\\\\nAuthors: Konstantin Todorov, Giovanni Colavizza\\\\nSummary: Neural language models are the backbone of modern-day natural language\\\\nprocessing applications. Their use on textual heritage collections which have\\\\nundergone Optical Character Recognition (OCR) is therefore also increasing.\\\\nNevertheless, our understanding of the impact OCR noise could have on language\\\\nmodels is still limited. We perform an assessment of the impact OCR noise has\\\\non a variety of language models, using data in Dutch, English, French and\\\\nGerman. We find that OCR noise poses a significant obstacle to language\\\\nmodelling, with language models increasingly diverging from their noiseless\\\\ntargets as OCR quality lowers. In the presence of small corpora, simpler models\\\\nincluding PPMI and Word2Vec consistently outperform transformer-based models in\\\\nthis respect.\\\\n\\\\nPublished: 2024-12-09\\\\nTitle: Small Languages, Big Models: A Study of Continual Training on Languages of Norway\\\\nAuthors: David Samuel, Vladislav Mikhailov, Erik Velldal, Lilja \\\\u00d8vrelid, Lucas Georges Gabriel Charpentier, Andrey Kutuzov\\\\nSummary: Training large language models requires vast amounts of data, posing a\\\\nchallenge for less widely spoken languages like Norwegian and even more so for\\\\ntruly low-resource languages like S\\\\\\\\\\'ami. To address this issue, we present a\\\\nnovel three-stage continual training approach. We also experiment with\\\\ncombining causal and masked language modeling to get more flexible models.\\\\nBased on our findings, we train, evaluate, and openly release a new large\\\\ngenerative language model for Norwegian Bokm\\\\\\\\r{a}l, Nynorsk, and Northern\\\\nS\\\\\\\\\\'ami with 11.4 billion parameters: NorMistral-11B.\\\\n\\\\nPublished: 2024-09-26\\\\nTitle: Enhancing elusive clues in knowledge learning by contrasting attention of language models\\\\nAuthors: Jian Gao, Xiao Zhang, Ji Wu, Miao Li\\\\nSummary: Causal language models acquire vast amount of knowledge from general text\\\\ncorpus during pretraining, but the efficiency of knowledge learning is known to\\\\nbe unsatisfactory, especially when learning from knowledge-dense and\\\\nsmall-sized corpora. The deficiency can come from long-distance dependencies\\\\nwhich are hard to capture by language models, and overfitting to co-occurrence\\\\npatterns and distracting clues in the training text. To address these issues,\\\\nthe paper proposes a method to enhance knowledge learning during language model\\\\npretraining, by enhancing elusive but important clues in text discovered by the\\\\nlanguage model themselves. We found that larger language models pay more\\\\nattention to non-obvious but important clues, which are often overlooked by\\\\nsmaller language models. Therefore, we can identify these clues by contrasting\\\\nthe attention weights of large and small language models. We use the identified\\\\nclues as a guide to perform token-dropout data augmentation on the training\\\\ntext, and observed a significant boost in both small and large models\\'\\\\nperformance in fact memorization. This shows that the behavior contrast between\\\\nmore and less-performant language models contains important clues for knowledge\\\\nlearning, and it can be ``amplified\\\\\" for a straight-forward improvement in\\\\nknowledge learning efficiency.\"', name='arxiv', tool_call_id='call_RofKJ0sqRlA3BluuhZecGTww')],\n",
      "           'intermediate_step': ['[ToolMessage(content=\\'\"Published: '\n",
      "                                 '2022-01-26\\\\\\\\nTitle: An Assessment of the '\n",
      "                                 'Impact of OCR Noise on Language '\n",
      "                                 'Models\\\\\\\\nAuthors: Konstantin Todorov, '\n",
      "                                 'Giovanni Colavizza\\\\\\\\nSummary: Neural '\n",
      "                                 'language models are the backbone of '\n",
      "                                 'modern-day natural language\\\\\\\\nprocessing '\n",
      "                                 'applications. Their use on textual heritage '\n",
      "                                 'collections which have\\\\\\\\nundergone Optical '\n",
      "                                 'Character Recognition (OCR) is therefore '\n",
      "                                 'also increasing.\\\\\\\\nNevertheless, our '\n",
      "                                 'understanding of the impact OCR noise could '\n",
      "                                 'have on language\\\\\\\\nmodels is still '\n",
      "                                 'limited. We perform an assessment of the '\n",
      "                                 'impact OCR noise has\\\\\\\\non a variety of '\n",
      "                                 'language models, using data in Dutch, '\n",
      "                                 'English, French and\\\\\\\\nGerman. We find that '\n",
      "                                 'OCR noise poses a significant obstacle to '\n",
      "                                 'language\\\\\\\\nmodelling, with language models '\n",
      "                                 'increasingly diverging from their '\n",
      "                                 'noiseless\\\\\\\\ntargets as OCR quality lowers. '\n",
      "                                 'In the presence of small corpora, simpler '\n",
      "                                 'models\\\\\\\\nincluding PPMI and Word2Vec '\n",
      "                                 'consistently outperform transformer-based '\n",
      "                                 'models in\\\\\\\\nthis '\n",
      "                                 'respect.\\\\\\\\n\\\\\\\\nPublished: '\n",
      "                                 '2024-12-09\\\\\\\\nTitle: Small Languages, Big '\n",
      "                                 'Models: A Study of Continual Training on '\n",
      "                                 'Languages of Norway\\\\\\\\nAuthors: David '\n",
      "                                 'Samuel, Vladislav Mikhailov, Erik Velldal, '\n",
      "                                 'Lilja \\\\\\\\u00d8vrelid, Lucas Georges Gabriel '\n",
      "                                 'Charpentier, Andrey Kutuzov\\\\\\\\nSummary: '\n",
      "                                 'Training large language models requires vast '\n",
      "                                 'amounts of data, posing a\\\\\\\\nchallenge for '\n",
      "                                 'less widely spoken languages like Norwegian '\n",
      "                                 'and even more so for\\\\\\\\ntruly low-resource '\n",
      "                                 \"languages like S\\\\\\\\\\\\\\\\\\\\'ami. To address \"\n",
      "                                 'this issue, we present a\\\\\\\\nnovel '\n",
      "                                 'three-stage continual training approach. We '\n",
      "                                 'also experiment with\\\\\\\\ncombining causal '\n",
      "                                 'and masked language modeling to get more '\n",
      "                                 'flexible models.\\\\\\\\nBased on our findings, '\n",
      "                                 'we train, evaluate, and openly release a new '\n",
      "                                 'large\\\\\\\\ngenerative language model for '\n",
      "                                 'Norwegian Bokm\\\\\\\\\\\\\\\\r{a}l, Nynorsk, and '\n",
      "                                 \"Northern\\\\\\\\nS\\\\\\\\\\\\\\\\\\\\'ami with 11.4 \"\n",
      "                                 'billion parameters: '\n",
      "                                 'NorMistral-11B.\\\\\\\\n\\\\\\\\nPublished: '\n",
      "                                 '2024-09-26\\\\\\\\nTitle: Enhancing elusive '\n",
      "                                 'clues in knowledge learning by contrasting '\n",
      "                                 'attention of language models\\\\\\\\nAuthors: '\n",
      "                                 'Jian Gao, Xiao Zhang, Ji Wu, Miao '\n",
      "                                 'Li\\\\\\\\nSummary: Causal language models '\n",
      "                                 'acquire vast amount of knowledge from '\n",
      "                                 'general text\\\\\\\\ncorpus during pretraining, '\n",
      "                                 'but the efficiency of knowledge learning is '\n",
      "                                 'known to\\\\\\\\nbe unsatisfactory, especially '\n",
      "                                 'when learning from knowledge-dense '\n",
      "                                 'and\\\\\\\\nsmall-sized corpora. The deficiency '\n",
      "                                 'can come from long-distance '\n",
      "                                 'dependencies\\\\\\\\nwhich are hard to capture '\n",
      "                                 'by language models, and overfitting to '\n",
      "                                 'co-occurrence\\\\\\\\npatterns and distracting '\n",
      "                                 'clues in the training text. To address these '\n",
      "                                 'issues,\\\\\\\\nthe paper proposes a method to '\n",
      "                                 'enhance knowledge learning during language '\n",
      "                                 'model\\\\\\\\npretraining, by enhancing elusive '\n",
      "                                 'but important clues in text discovered by '\n",
      "                                 'the\\\\\\\\nlanguage model themselves. We found '\n",
      "                                 'that larger language models pay '\n",
      "                                 'more\\\\\\\\nattention to non-obvious but '\n",
      "                                 'important clues, which are often overlooked '\n",
      "                                 'by\\\\\\\\nsmaller language models. Therefore, '\n",
      "                                 'we can identify these clues by '\n",
      "                                 'contrasting\\\\\\\\nthe attention weights of '\n",
      "                                 'large and small language models. We use the '\n",
      "                                 'identified\\\\\\\\nclues as a guide to perform '\n",
      "                                 'token-dropout data augmentation on the '\n",
      "                                 'training\\\\\\\\ntext, and observed a '\n",
      "                                 'significant boost in both small and large '\n",
      "                                 \"models\\\\'\\\\\\\\nperformance in fact \"\n",
      "                                 'memorization. This shows that the behavior '\n",
      "                                 'contrast between\\\\\\\\nmore and '\n",
      "                                 'less-performant language models contains '\n",
      "                                 'important clues for knowledge\\\\\\\\nlearning, '\n",
      "                                 'and it can be ``amplified\\\\\\\\\" for a '\n",
      "                                 'straight-forward improvement '\n",
      "                                 'in\\\\\\\\nknowledge learning efficiency.\"\\', '\n",
      "                                 \"name='arxiv', \"\n",
      "                                 \"tool_call_id='call_RofKJ0sqRlA3BluuhZecGTww')]\"]}}\n",
      "\"Published: 2022-01-26\\nTitle: An Assessment of the Impact of OCR Noise on Language Models\\nAuthors: Konstantin Todorov, Giovanni Colavizza\\nSummary: Neural language models are the backbone of modern-day natural language\\nprocessing applications. Their use on textual heritage collections which have\\nundergone Optical Character Recognition (OCR) is therefore also increasing.\\nNevertheless, our understanding of the impact OCR noise could have on language\\nmodels is still limited. We perform an assessment of the impact OCR noise has\\non a variety of language models, using data in Dutch, English, French and\\nGerman. We find that OCR noise poses a significant obstacle to language\\nmodelling, with language models increasingly diverging from their noiseless\\ntargets as OCR quality lowers. In the presence of small corpora, simpler models\\nincluding PPMI and Word2Vec consistently outperform transformer-based models in\\nthis respect.\\n\\nPublished: 2024-12-09\\nTitle: Small Languages, Big Models: A Study of Continual Training on Languages of Norway\\nAuthors: David Samuel, Vladislav Mikhailov, Erik Velldal, Lilja \\u00d8vrelid, Lucas Georges Gabriel Charpentier, Andrey Kutuzov\\nSummary: Training large language models requires vast amounts of data, posing a\\nchallenge for less widely spoken languages like Norwegian and even more so for\\ntruly low-resource languages like S\\\\'ami. To address this issue, we present a\\nnovel three-stage continual training approach. We also experiment with\\ncombining causal and masked language modeling to get more flexible models.\\nBased on our findings, we train, evaluate, and openly release a new large\\ngenerative language model for Norwegian Bokm\\\\r{a}l, Nynorsk, and Northern\\nS\\\\'ami with 11.4 billion parameters: NorMistral-11B.\\n\\nPublished: 2024-09-26\\nTitle: Enhancing elusive clues in knowledge learning by contrasting attention of language models\\nAuthors: Jian Gao, Xiao Zhang, Ji Wu, Miao Li\\nSummary: Causal language models acquire vast amount of knowledge from general text\\ncorpus during pretraining, but the efficiency of knowledge learning is known to\\nbe unsatisfactory, especially when learning from knowledge-dense and\\nsmall-sized corpora. The deficiency can come from long-distance dependencies\\nwhich are hard to capture by language models, and overfitting to co-occurrence\\npatterns and distracting clues in the training text. To address these issues,\\nthe paper proposes a method to enhance knowledge learning during language model\\npretraining, by enhancing elusive but important clues in text discovered by the\\nlanguage model themselves. We found that larger language models pay more\\nattention to non-obvious but important clues, which are often overlooked by\\nsmaller language models. Therefore, we can identify these clues by contrasting\\nthe attention weights of large and small language models. We use the identified\\nclues as a guide to perform token-dropout data augmentation on the training\\ntext, and observed a significant boost in both small and large models'\\nperformance in fact memorization. This shows that the behavior contrast between\\nmore and less-performant language models contains important clues for knowledge\\nlearning, and it can be ``amplified\\\" for a straight-forward improvement in\\nknowledge learning efficiency.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----research node----\n",
      "{'input': 'What are the recent papers on Small Language Models?', 'agent_outcome': [ToolMessage(content='\"Published: 2022-01-26\\\\nTitle: An Assessment of the Impact of OCR Noise on Language Models\\\\nAuthors: Konstantin Todorov, Giovanni Colavizza\\\\nSummary: Neural language models are the backbone of modern-day natural language\\\\nprocessing applications. Their use on textual heritage collections which have\\\\nundergone Optical Character Recognition (OCR) is therefore also increasing.\\\\nNevertheless, our understanding of the impact OCR noise could have on language\\\\nmodels is still limited. We perform an assessment of the impact OCR noise has\\\\non a variety of language models, using data in Dutch, English, French and\\\\nGerman. We find that OCR noise poses a significant obstacle to language\\\\nmodelling, with language models increasingly diverging from their noiseless\\\\ntargets as OCR quality lowers. In the presence of small corpora, simpler models\\\\nincluding PPMI and Word2Vec consistently outperform transformer-based models in\\\\nthis respect.\\\\n\\\\nPublished: 2024-12-09\\\\nTitle: Small Languages, Big Models: A Study of Continual Training on Languages of Norway\\\\nAuthors: David Samuel, Vladislav Mikhailov, Erik Velldal, Lilja \\\\u00d8vrelid, Lucas Georges Gabriel Charpentier, Andrey Kutuzov\\\\nSummary: Training large language models requires vast amounts of data, posing a\\\\nchallenge for less widely spoken languages like Norwegian and even more so for\\\\ntruly low-resource languages like S\\\\\\\\\\'ami. To address this issue, we present a\\\\nnovel three-stage continual training approach. We also experiment with\\\\ncombining causal and masked language modeling to get more flexible models.\\\\nBased on our findings, we train, evaluate, and openly release a new large\\\\ngenerative language model for Norwegian Bokm\\\\\\\\r{a}l, Nynorsk, and Northern\\\\nS\\\\\\\\\\'ami with 11.4 billion parameters: NorMistral-11B.\\\\n\\\\nPublished: 2024-09-26\\\\nTitle: Enhancing elusive clues in knowledge learning by contrasting attention of language models\\\\nAuthors: Jian Gao, Xiao Zhang, Ji Wu, Miao Li\\\\nSummary: Causal language models acquire vast amount of knowledge from general text\\\\ncorpus during pretraining, but the efficiency of knowledge learning is known to\\\\nbe unsatisfactory, especially when learning from knowledge-dense and\\\\nsmall-sized corpora. The deficiency can come from long-distance dependencies\\\\nwhich are hard to capture by language models, and overfitting to co-occurrence\\\\npatterns and distracting clues in the training text. To address these issues,\\\\nthe paper proposes a method to enhance knowledge learning during language model\\\\npretraining, by enhancing elusive but important clues in text discovered by the\\\\nlanguage model themselves. We found that larger language models pay more\\\\nattention to non-obvious but important clues, which are often overlooked by\\\\nsmaller language models. Therefore, we can identify these clues by contrasting\\\\nthe attention weights of large and small language models. We use the identified\\\\nclues as a guide to perform token-dropout data augmentation on the training\\\\ntext, and observed a significant boost in both small and large models\\'\\\\nperformance in fact memorization. This shows that the behavior contrast between\\\\nmore and less-performant language models contains important clues for knowledge\\\\nlearning, and it can be ``amplified\\\\\" for a straight-forward improvement in\\\\nknowledge learning efficiency.\"', name='arxiv', tool_call_id='call_RofKJ0sqRlA3BluuhZecGTww')], 'intermediate_step': ['[ToolMessage(content=\\'\"Published: 2022-01-26\\\\\\\\nTitle: An Assessment of the Impact of OCR Noise on Language Models\\\\\\\\nAuthors: Konstantin Todorov, Giovanni Colavizza\\\\\\\\nSummary: Neural language models are the backbone of modern-day natural language\\\\\\\\nprocessing applications. Their use on textual heritage collections which have\\\\\\\\nundergone Optical Character Recognition (OCR) is therefore also increasing.\\\\\\\\nNevertheless, our understanding of the impact OCR noise could have on language\\\\\\\\nmodels is still limited. We perform an assessment of the impact OCR noise has\\\\\\\\non a variety of language models, using data in Dutch, English, French and\\\\\\\\nGerman. We find that OCR noise poses a significant obstacle to language\\\\\\\\nmodelling, with language models increasingly diverging from their noiseless\\\\\\\\ntargets as OCR quality lowers. In the presence of small corpora, simpler models\\\\\\\\nincluding PPMI and Word2Vec consistently outperform transformer-based models in\\\\\\\\nthis respect.\\\\\\\\n\\\\\\\\nPublished: 2024-12-09\\\\\\\\nTitle: Small Languages, Big Models: A Study of Continual Training on Languages of Norway\\\\\\\\nAuthors: David Samuel, Vladislav Mikhailov, Erik Velldal, Lilja \\\\\\\\u00d8vrelid, Lucas Georges Gabriel Charpentier, Andrey Kutuzov\\\\\\\\nSummary: Training large language models requires vast amounts of data, posing a\\\\\\\\nchallenge for less widely spoken languages like Norwegian and even more so for\\\\\\\\ntruly low-resource languages like S\\\\\\\\\\\\\\\\\\\\\\'ami. To address this issue, we present a\\\\\\\\nnovel three-stage continual training approach. We also experiment with\\\\\\\\ncombining causal and masked language modeling to get more flexible models.\\\\\\\\nBased on our findings, we train, evaluate, and openly release a new large\\\\\\\\ngenerative language model for Norwegian Bokm\\\\\\\\\\\\\\\\r{a}l, Nynorsk, and Northern\\\\\\\\nS\\\\\\\\\\\\\\\\\\\\\\'ami with 11.4 billion parameters: NorMistral-11B.\\\\\\\\n\\\\\\\\nPublished: 2024-09-26\\\\\\\\nTitle: Enhancing elusive clues in knowledge learning by contrasting attention of language models\\\\\\\\nAuthors: Jian Gao, Xiao Zhang, Ji Wu, Miao Li\\\\\\\\nSummary: Causal language models acquire vast amount of knowledge from general text\\\\\\\\ncorpus during pretraining, but the efficiency of knowledge learning is known to\\\\\\\\nbe unsatisfactory, especially when learning from knowledge-dense and\\\\\\\\nsmall-sized corpora. The deficiency can come from long-distance dependencies\\\\\\\\nwhich are hard to capture by language models, and overfitting to co-occurrence\\\\\\\\npatterns and distracting clues in the training text. To address these issues,\\\\\\\\nthe paper proposes a method to enhance knowledge learning during language model\\\\\\\\npretraining, by enhancing elusive but important clues in text discovered by the\\\\\\\\nlanguage model themselves. We found that larger language models pay more\\\\\\\\nattention to non-obvious but important clues, which are often overlooked by\\\\\\\\nsmaller language models. Therefore, we can identify these clues by contrasting\\\\\\\\nthe attention weights of large and small language models. We use the identified\\\\\\\\nclues as a guide to perform token-dropout data augmentation on the training\\\\\\\\ntext, and observed a significant boost in both small and large models\\\\\\'\\\\\\\\nperformance in fact memorization. This shows that the behavior contrast between\\\\\\\\nmore and less-performant language models contains important clues for knowledge\\\\\\\\nlearning, and it can be ``amplified\\\\\\\\\" for a straight-forward improvement in\\\\\\\\nknowledge learning efficiency.\"\\', name=\\'arxiv\\', tool_call_id=\\'call_RofKJ0sqRlA3BluuhZecGTww\\')]']}\n",
      "input_variables=['input', 'intermediate_step'] input_types={'intermediate_step': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} partial_variables={'tool_names': 'arxiv'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['tool_names'], template='You are a helpful AI research assistant, Use the appropriate search tools to progress towards finding the relevant results. Once you have the relevant search results, summarise them to answer the user query.\\nYou have access to the following search tools: {tool_names}.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='\\nUser Query: {input}')), MessagesPlaceholder(variable_name='intermediate_step')]\n",
      "----router----\n",
      "{'agent_outcome': [AIMessage(content='Here are some recent papers related to Small Language Models:\\n\\n1. **Title**: An Assessment of the Impact of OCR Noise on Language Models  \\n   **Authors**: Konstantin Todorov, Giovanni Colavizza  \\n   **Published**: January 26, 2022  \\n   **Summary**: This paper investigates how Optical Character Recognition (OCR) noise affects various language models. The study finds that OCR noise significantly hinders language modeling, particularly impacting transformer-based models when trained on small corpora. Simpler models like PPMI and Word2Vec perform better under these conditions.\\n\\n2. **Title**: Small Languages, Big Models: A Study of Continual Training on Languages of Norway  \\n   **Authors**: David Samuel, Vladislav Mikhailov, Erik Velldal, Lilja Øvrelid, Lucas Georges Gabriel Charpentier, Andrey Kutuzov  \\n   **Published**: December 9, 2024  \\n   **Summary**: This research addresses the challenge of training large language models for less widely spoken languages, such as Norwegian and low-resource languages like Sámi. The authors propose a three-stage continual training approach and introduce a new generative language model, NorMistral-11B, with 11.4 billion parameters, specifically for Norwegian Bokmål, Nynorsk, and Northern Sámi.\\n\\n3. **Title**: Enhancing elusive clues in knowledge learning by contrasting attention of language models  \\n   **Authors**: Jian Gao, Xiao Zhang, Ji Wu, Miao Li  \\n   **Published**: September 26, 2024  \\n   **Summary**: This paper explores the efficiency of knowledge learning in causal language models, particularly when dealing with small-sized corpora. It proposes a method to enhance knowledge learning by focusing on important but often overlooked clues in the text. The study shows that larger models can identify these clues better than smaller ones, leading to improved performance through data augmentation techniques.\\n\\nThese papers highlight various aspects of small language models, including their performance under noise, training methodologies for low-resource languages, and strategies for enhancing knowledge learning.', response_metadata={'token_usage': {'completion_tokens': 429, 'prompt_tokens': 936, 'total_tokens': 1365, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-300251f8-1d58-44ef-91d0-9af55218d0a9-0', usage_metadata={'input_tokens': 936, 'output_tokens': 429, 'total_tokens': 1365})], 'input': 'What are the recent papers on Small Language Models?', 'intermediate_step': ['[ToolMessage(content=\\'\"Published: 2022-01-26\\\\\\\\nTitle: An Assessment of the Impact of OCR Noise on Language Models\\\\\\\\nAuthors: Konstantin Todorov, Giovanni Colavizza\\\\\\\\nSummary: Neural language models are the backbone of modern-day natural language\\\\\\\\nprocessing applications. Their use on textual heritage collections which have\\\\\\\\nundergone Optical Character Recognition (OCR) is therefore also increasing.\\\\\\\\nNevertheless, our understanding of the impact OCR noise could have on language\\\\\\\\nmodels is still limited. We perform an assessment of the impact OCR noise has\\\\\\\\non a variety of language models, using data in Dutch, English, French and\\\\\\\\nGerman. We find that OCR noise poses a significant obstacle to language\\\\\\\\nmodelling, with language models increasingly diverging from their noiseless\\\\\\\\ntargets as OCR quality lowers. In the presence of small corpora, simpler models\\\\\\\\nincluding PPMI and Word2Vec consistently outperform transformer-based models in\\\\\\\\nthis respect.\\\\\\\\n\\\\\\\\nPublished: 2024-12-09\\\\\\\\nTitle: Small Languages, Big Models: A Study of Continual Training on Languages of Norway\\\\\\\\nAuthors: David Samuel, Vladislav Mikhailov, Erik Velldal, Lilja \\\\\\\\u00d8vrelid, Lucas Georges Gabriel Charpentier, Andrey Kutuzov\\\\\\\\nSummary: Training large language models requires vast amounts of data, posing a\\\\\\\\nchallenge for less widely spoken languages like Norwegian and even more so for\\\\\\\\ntruly low-resource languages like S\\\\\\\\\\\\\\\\\\\\\\'ami. To address this issue, we present a\\\\\\\\nnovel three-stage continual training approach. We also experiment with\\\\\\\\ncombining causal and masked language modeling to get more flexible models.\\\\\\\\nBased on our findings, we train, evaluate, and openly release a new large\\\\\\\\ngenerative language model for Norwegian Bokm\\\\\\\\\\\\\\\\r{a}l, Nynorsk, and Northern\\\\\\\\nS\\\\\\\\\\\\\\\\\\\\\\'ami with 11.4 billion parameters: NorMistral-11B.\\\\\\\\n\\\\\\\\nPublished: 2024-09-26\\\\\\\\nTitle: Enhancing elusive clues in knowledge learning by contrasting attention of language models\\\\\\\\nAuthors: Jian Gao, Xiao Zhang, Ji Wu, Miao Li\\\\\\\\nSummary: Causal language models acquire vast amount of knowledge from general text\\\\\\\\ncorpus during pretraining, but the efficiency of knowledge learning is known to\\\\\\\\nbe unsatisfactory, especially when learning from knowledge-dense and\\\\\\\\nsmall-sized corpora. The deficiency can come from long-distance dependencies\\\\\\\\nwhich are hard to capture by language models, and overfitting to co-occurrence\\\\\\\\npatterns and distracting clues in the training text. To address these issues,\\\\\\\\nthe paper proposes a method to enhance knowledge learning during language model\\\\\\\\npretraining, by enhancing elusive but important clues in text discovered by the\\\\\\\\nlanguage model themselves. We found that larger language models pay more\\\\\\\\nattention to non-obvious but important clues, which are often overlooked by\\\\\\\\nsmaller language models. Therefore, we can identify these clues by contrasting\\\\\\\\nthe attention weights of large and small language models. We use the identified\\\\\\\\nclues as a guide to perform token-dropout data augmentation on the training\\\\\\\\ntext, and observed a significant boost in both small and large models\\\\\\'\\\\\\\\nperformance in fact memorization. This shows that the behavior contrast between\\\\\\\\nmore and less-performant language models contains important clues for knowledge\\\\\\\\nlearning, and it can be ``amplified\\\\\\\\\" for a straight-forward improvement in\\\\\\\\nknowledge learning efficiency.\"\\', name=\\'arxiv\\', tool_call_id=\\'call_RofKJ0sqRlA3BluuhZecGTww\\')]']}\n",
      "content='Here are some recent papers related to Small Language Models:\\n\\n1. **Title**: An Assessment of the Impact of OCR Noise on Language Models  \\n   **Authors**: Konstantin Todorov, Giovanni Colavizza  \\n   **Published**: January 26, 2022  \\n   **Summary**: This paper investigates how Optical Character Recognition (OCR) noise affects various language models. The study finds that OCR noise significantly hinders language modeling, particularly impacting transformer-based models when trained on small corpora. Simpler models like PPMI and Word2Vec perform better under these conditions.\\n\\n2. **Title**: Small Languages, Big Models: A Study of Continual Training on Languages of Norway  \\n   **Authors**: David Samuel, Vladislav Mikhailov, Erik Velldal, Lilja Øvrelid, Lucas Georges Gabriel Charpentier, Andrey Kutuzov  \\n   **Published**: December 9, 2024  \\n   **Summary**: This research addresses the challenge of training large language models for less widely spoken languages, such as Norwegian and low-resource languages like Sámi. The authors propose a three-stage continual training approach and introduce a new generative language model, NorMistral-11B, with 11.4 billion parameters, specifically for Norwegian Bokmål, Nynorsk, and Northern Sámi.\\n\\n3. **Title**: Enhancing elusive clues in knowledge learning by contrasting attention of language models  \\n   **Authors**: Jian Gao, Xiao Zhang, Ji Wu, Miao Li  \\n   **Published**: September 26, 2024  \\n   **Summary**: This paper explores the efficiency of knowledge learning in causal language models, particularly when dealing with small-sized corpora. It proposes a method to enhance knowledge learning by focusing on important but often overlooked clues in the text. The study shows that larger models can identify these clues better than smaller ones, leading to improved performance through data augmentation techniques.\\n\\nThese papers highlight various aspects of small language models, including their performance under noise, training methodologies for low-resource languages, and strategies for enhancing knowledge learning.' response_metadata={'token_usage': {'completion_tokens': 429, 'prompt_tokens': 936, 'total_tokens': 1365, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}} id='run-300251f8-1d58-44ef-91d0-9af55218d0a9-0' usage_metadata={'input_tokens': 936, 'output_tokens': 429, 'total_tokens': 1365}\n",
      "{'research': {'agent_outcome': [AIMessage(content='Here are some recent papers related to Small Language Models:\\n\\n1. **Title**: An Assessment of the Impact of OCR Noise on Language Models  \\n   **Authors**: Konstantin Todorov, Giovanni Colavizza  \\n   **Published**: January 26, 2022  \\n   **Summary**: This paper investigates how Optical Character Recognition (OCR) noise affects various language models. The study finds that OCR noise significantly hinders language modeling, particularly impacting transformer-based models when trained on small corpora. Simpler models like PPMI and Word2Vec perform better under these conditions.\\n\\n2. **Title**: Small Languages, Big Models: A Study of Continual Training on Languages of Norway  \\n   **Authors**: David Samuel, Vladislav Mikhailov, Erik Velldal, Lilja Øvrelid, Lucas Georges Gabriel Charpentier, Andrey Kutuzov  \\n   **Published**: December 9, 2024  \\n   **Summary**: This research addresses the challenge of training large language models for less widely spoken languages, such as Norwegian and low-resource languages like Sámi. The authors propose a three-stage continual training approach and introduce a new generative language model, NorMistral-11B, with 11.4 billion parameters, specifically for Norwegian Bokmål, Nynorsk, and Northern Sámi.\\n\\n3. **Title**: Enhancing elusive clues in knowledge learning by contrasting attention of language models  \\n   **Authors**: Jian Gao, Xiao Zhang, Ji Wu, Miao Li  \\n   **Published**: September 26, 2024  \\n   **Summary**: This paper explores the efficiency of knowledge learning in causal language models, particularly when dealing with small-sized corpora. It proposes a method to enhance knowledge learning by focusing on important but often overlooked clues in the text. The study shows that larger models can identify these clues better than smaller ones, leading to improved performance through data augmentation techniques.\\n\\nThese papers highlight various aspects of small language models, including their performance under noise, training methodologies for low-resource languages, and strategies for enhancing knowledge learning.', response_metadata={'token_usage': {'completion_tokens': 429, 'prompt_tokens': 936, 'total_tokens': 1365, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-300251f8-1d58-44ef-91d0-9af55218d0a9-0', usage_metadata={'input_tokens': 936, 'output_tokens': 429, 'total_tokens': 1365})]}}\n",
      "Here are some recent papers related to Small Language Models:\n",
      "\n",
      "1. **Title**: An Assessment of the Impact of OCR Noise on Language Models  \n",
      "   **Authors**: Konstantin Todorov, Giovanni Colavizza  \n",
      "   **Published**: January 26, 2022  \n",
      "   **Summary**: This paper investigates how Optical Character Recognition (OCR) noise affects various language models. The study finds that OCR noise significantly hinders language modeling, particularly impacting transformer-based models when trained on small corpora. Simpler models like PPMI and Word2Vec perform better under these conditions.\n",
      "\n",
      "2. **Title**: Small Languages, Big Models: A Study of Continual Training on Languages of Norway  \n",
      "   **Authors**: David Samuel, Vladislav Mikhailov, Erik Velldal, Lilja Øvrelid, Lucas Georges Gabriel Charpentier, Andrey Kutuzov  \n",
      "   **Published**: December 9, 2024  \n",
      "   **Summary**: This research addresses the challenge of training large language models for less widely spoken languages, such as Norwegian and low-resource languages like Sámi. The authors propose a three-stage continual training approach and introduce a new generative language model, NorMistral-11B, with 11.4 billion parameters, specifically for Norwegian Bokmål, Nynorsk, and Northern Sámi.\n",
      "\n",
      "3. **Title**: Enhancing elusive clues in knowledge learning by contrasting attention of language models  \n",
      "   **Authors**: Jian Gao, Xiao Zhang, Ji Wu, Miao Li  \n",
      "   **Published**: September 26, 2024  \n",
      "   **Summary**: This paper explores the efficiency of knowledge learning in causal language models, particularly when dealing with small-sized corpora. It proposes a method to enhance knowledge learning by focusing on important but often overlooked clues in the text. The study shows that larger models can identify these clues better than smaller ones, leading to improved performance through data augmentation techniques.\n",
      "\n",
      "These papers highlight various aspects of small language models, including their performance under noise, training methodologies for low-resource languages, and strategies for enhancing knowledge learning.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"input\": \"What are the recent papers on Small Language Models?\",\n",
    "}\n",
    "\n",
    "state = AgentState(**inputs)\n",
    "for s in app.stream(input=state):\n",
    "    pprint.pp(s)\n",
    "    print(list(s.values())[0]['agent_outcome'][0].content)\n",
    "    print(\"-----\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dict.values>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
